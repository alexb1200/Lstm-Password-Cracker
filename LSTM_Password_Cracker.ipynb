{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LSTM_Password_Cracker.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "punL79CN7Ox6"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "_ckMIh7O7s6D"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph5eir3Pf-3z"
      },
      "source": [
        "# Constructing a Text Generation Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5Uhzt6vVIB2"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GbGfr_oLCat"
      },
      "source": [
        "Using most of the techniques you've already learned, it's now possible to generate new text by predicting the next word that follows a given seed word. To practice this method, we'll use the [Kaggle Song Lyrics Dataset](https://www.kaggle.com/mousehead/songlyrics)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aHK2CYygXom"
      },
      "source": [
        "## Import TensorFlow and related functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LmLTREBf5ng"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Other imports for processing data\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmLTO_dpgge9"
      },
      "source": [
        "## Get the Dataset\n",
        "\n",
        "As noted above, we'll utilize the [Song Lyrics dataset](https://www.kaggle.com/mousehead/songlyrics) on Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Bf5FVHfganK"
      },
      "source": [
        "# !wget --no-check-certificate \\\n",
        "#     https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 \\\n",
        "#     -O /tmp/songdata.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu1BTzMIS1oy"
      },
      "source": [
        "## **First 10 Songs**\n",
        "\n",
        "Let's first look at just 10 songs from the dataset, and see how things perform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmb9rGaAUDO-"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "Let's perform some basic preprocessing to get rid of punctuation and make everything lowercase. We'll then split the lyrics up by line and tokenize the lyrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AVAvyF_Vuh5"
      },
      "source": [
        "def tokenize_corpus(corpus, num_words=-1):\n",
        "  # Fit a Tokenizer on the corpus\n",
        "  if num_words > -1:\n",
        "    tokenizer = Tokenizer(num_words=num_words)\n",
        "  else:\n",
        "    tokenizer = Tokenizer(filters=\" \", lower=False, char_level=True)\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  return tokenizer\n",
        "\n",
        "def create_lyrics_corpus(dataset, field):\n",
        "  # Remove all other punctuation\n",
        "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
        "  # Make it lowercase\n",
        "  dataset[field] = dataset[field].str.lower()\n",
        "  # Make it one long string to split by line\n",
        "  lyrics = dataset[field].str.cat()\n",
        "  corpus = lyrics.split('\\n')\n",
        "  # Remove any trailing whitespace\n",
        "  for l in range(len(corpus)):\n",
        "    corpus[l] = corpus[l].rstrip()\n",
        "  # Remove any empty lines\n",
        "  corpus = [l for l in corpus if l != '']\n",
        "\n",
        "  return corpus\n",
        "\n",
        "def create_pass_corpus(passwds):\n",
        "    #passwds=file.read_text()\n",
        "\n",
        "    return sorted(list(set(passwds)))\n",
        "    #corpus=passwds.split(\"\\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apcEXp7WhVBs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25c35ae0-f046-47f5-d6af-e795c14b5d34"
      },
      "source": [
        "# Read the dataset from csv - just first 10 songs for now\n",
        "#dataset = pd.read_csv('/tmp/songdata.csv', dtype=str)[:10]\n",
        "# Create the corpus using the 'text' column containing lyrics\n",
        "#corpus = create_lyrics_corpus(dataset, 'text')\n",
        "# Tokenize the corpus\n",
        "\n",
        "with open(\"200k_pass_sorted.txt\") as myfile:\n",
        "    pass_corpus = ['\\t'+next(myfile) for x in range(100000)]\n",
        "with open(\"200k_emails_sorted.txt\") as myfile:\n",
        "    email_corpus = ['\\t'+next(myfile)+'\\n' for x in range(100000)]\n",
        "    \n",
        "#print(corpus)\n",
        "\n",
        "\n",
        "\n",
        "pass_tokenizer = tokenize_corpus(pass_corpus)\n",
        "email_tokenizer= tokenize_corpus(email_corpus)\n",
        "\n",
        "total_pass_letters = len(pass_tokenizer.word_index) + 1\n",
        "total_email_letters= len(email_tokenizer.word_index) + 1\n",
        "print(pass_tokenizer.word_index)\n",
        "print(total_email_letters)\n",
        "print(total_pass_letters)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'\\t': 1, '\\n': 2, 'a': 3, '1': 4, 'e': 5, '0': 6, '2': 7, 'n': 8, 'i': 9, 'o': 10, 'r': 11, 's': 12, '9': 13, 'l': 14, 'm': 15, '3': 16, 't': 17, '8': 18, '5': 19, '4': 20, '7': 21, '6': 22, 'd': 23, 'c': 24, 'h': 25, 'b': 26, 'u': 27, 'k': 28, 'y': 29, 'g': 30, 'p': 31, 'f': 32, 'w': 33, 'j': 34, 'v': 35, 'z': 36, 'x': 37, 'A': 38, 'q': 39, 'E': 40, 'S': 41, 'B': 42, 'M': 43, 'L': 44, 'C': 45, 'R': 46, 'D': 47, 'I': 48, 'N': 49, 'T': 50, 'O': 51, 'H': 52, 'P': 53, 'J': 54, 'G': 55, 'K': 56, '!': 57, 'F': 58, '.': 59, 'U': 60, 'W': 61, 'Y': 62, '-': 63, 'V': 64, '_': 65, 'Z': 66, 'X': 67, 'Q': 68, ' ': 69, '@': 70, '#': 71, '?': 72, '/': 73, '*': 74, '&': 75, '$': 76, '%': 77, ';': 78, '=': 79, '+': 80, ']': 81, ',': 82, '[': 83, 'þ': 84, '`': 85, '^': 86, '\"': 87, \"'\": 88, 'Ø': 89, 'Ã': 90, 'Ÿ': 91, 'Å': 92, 'ý': 93, 'õ': 94, 'ú': 95, 'ß': 96, 'ø': 97, '�': 98, '\\\\': 99}\n",
            "85\n",
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9x68iN_X6FK"
      },
      "source": [
        "### Create Sequences and Labels\n",
        "\n",
        "After preprocessing, we next need to create sequences and labels. Creating the sequences themselves is similar to before with `texts_to_sequences`, but also including the use of [N-Grams](https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9); creating the labels will now utilize those sequences as well as utilize one-hot encoding over all potential output words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmlTsUqfikVO"
      },
      "source": [
        "\n",
        "def seqs(corpus, tokenizer, corpus_other, tokenizer_other):\n",
        "    sequences = []\n",
        "    sequences_other=[]\n",
        "    for i,line in enumerate(corpus):\n",
        "      token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "      token_list_other=tokenizer_other.texts_to_sequences([corpus_other[i]])[0]\n",
        "      for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        sequences.append(n_gram_sequence)\n",
        "        sequences_other.append(token_list_other)\n",
        "    return sequences,sequences_other\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# max_sequence_len_pass = max([len(seq) for seq in pass_corpus])\n",
        "# max_sequence_len_email = max([len(seq) for seq in email_corpus])\n",
        "# x,y=seqs(pass_corpus, pass_tokenizer, email_corpus,email_tokenizer)\n",
        "\n",
        "# print((x[1:10]))\n",
        "# print(y[1:10])\n",
        "#zip_seqs(email_corpus, pass_corpus, email_tokenizer, pass_tokenizer)\n",
        "\n",
        "# email_input_sequences = np.zeros(\n",
        "#     (len(email_corpus), max_sequence_len_email, total_email_letters), dtype=\"float32\"\n",
        "# )\n",
        "# pass_input_sequences = np.zeros(\n",
        "#     (len(pass_corpus), max_sequence_len_pass,total_pass_letters), dtype=\"float32\"\n",
        "# )\n",
        "# one_hot_labels = np.zeros(\n",
        "#     (len(pass_corpus), max_sequence_len_pass, total_pass_letters), dtype=\"float32\"\n",
        "# )\n",
        "\n",
        "\n",
        "# for i, (input_text, target_text) in enumerate(zip(email_corpus, pass_corpus)):\n",
        "#     for t, char in enumerate(input_text):\n",
        "#         try:\n",
        "#           email_input_sequences[i, t,email_tokenizer.word_index[char]]=1.0\n",
        "#         except:\n",
        "#           print(t)\n",
        "#     email_input_sequences[i, t + 1 :,email_tokenizer.word_index[\" \"]] = 1.0\n",
        "#     for t, char in enumerate(target_text):\n",
        "#         # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "#         pass_input_sequences[i, t, pass_tokenizer.word_index[char]] = 1.0\n",
        "#         if t > 0:\n",
        "#             # decoder_target_data will be ahead by one timestep\n",
        "#             # and will not include the start character.\n",
        "#             if(char=='\\t'):\n",
        "#               print(\"err\")\n",
        "#             one_hot_labels[i, t - 1, pass_tokenizer.word_index[char]] = 1.0\n",
        "#     pass_input_sequences[i, t + 1 :, pass_tokenizer.word_index[char]] = 1.0\n",
        "#     one_hot_labels[i, t:, pass_tokenizer.word_index[\" \"]] = 1.0\n",
        "\n",
        "\n",
        "#prev lstm!!!\n",
        "\n",
        "# email_seqs=seqs(email_corpus, email_tokenizer)\n",
        "# pass_seqs= seqs(pass_corpus, pass_tokenizer)\n",
        "\n",
        "pass_seqs, email_seqs=seqs(pass_corpus, pass_tokenizer, email_corpus,email_tokenizer)\n",
        "# # Pad sequences for equal input length \n",
        "max_sequence_len_pass = max([len(seq) for seq in pass_seqs])\n",
        "max_sequence_len_email = max([len(seq) for seq in email_seqs])\n",
        "pass_sequences = np.array(pad_sequences(pass_seqs, maxlen=max_sequence_len_pass, padding='pre'))\n",
        "email_sequences=np.array(pad_sequences(email_seqs, maxlen=max_sequence_len_email, padding='pre'))\n",
        "\n",
        "# Split sequences between the \"input\" sequence and \"output\" predicted word\n",
        "pass_input_sequences, pass_labels = pass_sequences[:,:-1], pass_sequences[:,-1]\n",
        "email_input_sequences= email_sequences\n",
        "\n",
        "\n",
        "# # One-hot encode the labels\n",
        "one_hot_labels = tf.keras.utils.to_categorical(pass_labels, num_classes=total_pass_letters)\n",
        "\n",
        "#NEED TO DO SUFFLING"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zsmu3aEId49i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b125e366-a4fd-45fe-aee0-5ed2ac0dd56d"
      },
      "source": [
        "# Check out how some of our data is being stored\n",
        "# The Tokenizer has just a single index per word\n",
        "print(pass_tokenizer.word_index['8'])\n",
        "print(pass_tokenizer.word_index['\\n'])\n",
        "print(pass_tokenizer.index_word[4])\n",
        "# Input sequences will have multiple indexes\n",
        "print(pass_input_sequences[5])\n",
        "print(pass_input_sequences[6])\n",
        "print(email_input_sequences[6])\n",
        "# And the one hot labels will be as long as the full spread of tokenized words\n",
        "print(one_hot_labels[5])\n",
        "print(one_hot_labels[6])\n",
        "\n",
        "print(len(pass_corpus))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18\n",
            "2\n",
            "1\n",
            "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  1 30  5 11  3 14]\n",
            "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  1 30  5 11  3 14 23]\n",
            "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  9 78 25 25 25 25 25 25 25 10 14  4  1  5  8  6\n",
            "  7  3  4  2  2]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0.]\n",
            "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0.]\n",
            "100000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1TAJMlmfO8r"
      },
      "source": [
        "### Train a Text Generation Model\n",
        "\n",
        "Building an RNN to train our text generation model will be very similar to the sentiment models you've built previously. The only real change necessary is to make sure to use Categorical instead of Binary Cross Entropy as the loss function - we could use Binary before since the sentiment was only 0 or 1, but now there are hundreds of categories.\n",
        "\n",
        "From there, we should also consider using *more* epochs than before, as text generation can take a little longer to converge than sentiment analysis, *and* we aren't working with all that much data yet. I'll set it at 200 epochs here since we're only use part of the dataset, and training will tail off quite a bit over that many epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1YXuxIqfygN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "94645d27-c2e4-488a-f056-15157ef3b353"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Input, Dropout\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow import keras\n",
        "latent_dim=256\n",
        "\n",
        "\n",
        "encoder_ins=keras.Input(shape=(max_sequence_len_email,))\n",
        "encoder_inputs= Embedding(total_email_letters, latent_dim, input_length= max_sequence_len_email-1)(encoder_ins)\n",
        "encoder= Bidirectional(LSTM(latent_dim,return_state=True, return_sequences=True))\n",
        "e2=Bidirectional(LSTM(latent_dim,return_state=True))\n",
        "e=encoder(encoder_inputs)\n",
        "z= e2(e)\n",
        "encoder_outputs, state_h, state_c, _1, _2=z\n",
        "encoder_outputs=Dropout(.4)(encoder_outputs)\n",
        "encoder_states=[state_h, _1, state_c, _2]\n",
        "\n",
        "decoder_ins=keras.Input(shape=(max_sequence_len_pass-1,))\n",
        "decoder_inputs=Embedding(total_pass_letters, latent_dim, input_length=max_sequence_len_pass-1)(decoder_ins) #Input(shape=(None, total_pass_letters))\n",
        "decoder=Bidirectional(LSTM(latent_dim, return_sequences=True, return_state=False)) #Bidirectional wrap?\n",
        "d2=Bidirectional(LSTM(latent_dim, return_sequences=False, return_state=False))\n",
        "\n",
        "\n",
        "d=decoder(decoder_inputs, initial_state= encoder_states)\n",
        "\n",
        "decoder_out=d2(d)\n",
        "decoder_out=Dropout(.4)(decoder_out)\n",
        "\n",
        "decoder_dense=Dense(total_pass_letters, activation='softmax')\n",
        "decoder_outs= decoder_dense(decoder_out)\n",
        "\n",
        "model=Model([encoder_ins,decoder_ins],decoder_outs)\n",
        "model.compile(optimizer= 'adam', loss='categorical_crossentropy',  metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "model.fit(([email_input_sequences,pass_input_sequences]), one_hot_labels, epochs=200, validation_split=0.2, verbose=1, batch_size=32)\n",
        "model.save('/content/drive/MyDrive/pass_seq2')\n",
        "\n",
        "\n",
        "\n",
        "# unconditional_model = Sequential()\n",
        "# unconditional_model.add(Embedding(total_pass_letters, 256, input_length=max_sequence_len_pass-1))\n",
        "# unconditional_model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "# unconditional_model.add(tf.keras.layers.Dropout(.4))\n",
        "# unconditional_model.add(Bidirectional(LSTM(256))) #shape here?\n",
        "# unconditional_model.add(tf.keras.layers.Dropout(.4))\n",
        "# unconditional_model.add(Dense(total_pass_letters, activation='softmax'))\n",
        "# unconditional_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# unconditional_model.summary()\n",
        "# history = model.fit(input_sequences, one_hot_labels, epochs=1, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 53)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 53, 256)      21760       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 36)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional (Bidirectional)   [(None, 53, 512), (N 1050624     embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 36, 256)      25600       input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) [(None, 512), (None, 1574912     bidirectional[0][0]              \n",
            "                                                                 bidirectional[0][1]              \n",
            "                                                                 bidirectional[0][2]              \n",
            "                                                                 bidirectional[0][3]              \n",
            "                                                                 bidirectional[0][4]              \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_2 (Bidirectional) (None, 36, 512)      1050624     embedding_1[0][0]                \n",
            "                                                                 bidirectional_1[0][1]            \n",
            "                                                                 bidirectional_1[0][3]            \n",
            "                                                                 bidirectional_1[0][2]            \n",
            "                                                                 bidirectional_1[0][4]            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_3 (Bidirectional) (None, 512)          1574912     bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 512)          0           bidirectional_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 100)          51300       dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 5,349,732\n",
            "Trainable params: 5,349,732\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "21780/21780 [==============================] - 851s 39ms/step - loss: 2.6587 - accuracy: 0.2560 - val_loss: 2.4475 - val_accuracy: 0.2896\n",
            "Epoch 2/200\n",
            "21780/21780 [==============================] - 829s 38ms/step - loss: 2.5002 - accuracy: 0.2960 - val_loss: 2.4059 - val_accuracy: 0.2992\n",
            "Epoch 3/200\n",
            " 3415/21780 [===>..........................] - ETA: 10:42 - loss: 2.4506 - accuracy: 0.3074"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-32f98bb725a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0memail_input_sequences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpass_input_sequences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/pass_seq2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8YpxW_zPiUF",
        "outputId": "86039417-9686-411a-8106-f3862f2578d1"
      },
      "source": [
        "model.fit(([email_input_sequences,pass_input_sequences]), one_hot_labels, epochs=200, validation_split=0.2, verbose=1, batch_size=32)\n",
        "model.save('/content/drive/MyDrive/pass_seq2')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "21780/21780 [==============================] - 838s 38ms/step - loss: 2.4316 - accuracy: 0.3143 - val_loss: 2.3844 - val_accuracy: 0.3097\n",
            "Epoch 2/200\n",
            "21780/21780 [==============================] - 843s 39ms/step - loss: 2.3967 - accuracy: 0.3236 - val_loss: 2.3796 - val_accuracy: 0.3122\n",
            "Epoch 3/200\n",
            "21780/21780 [==============================] - 840s 39ms/step - loss: 2.3705 - accuracy: 0.3295 - val_loss: 2.3689 - val_accuracy: 0.3138\n",
            "Epoch 4/200\n",
            "21780/21780 [==============================] - 843s 39ms/step - loss: 2.3530 - accuracy: 0.3343 - val_loss: 2.3656 - val_accuracy: 0.3152\n",
            "Epoch 5/200\n",
            "21780/21780 [==============================] - 847s 39ms/step - loss: 2.3395 - accuracy: 0.3372 - val_loss: 2.3643 - val_accuracy: 0.3175\n",
            "Epoch 6/200\n",
            "21780/21780 [==============================] - 843s 39ms/step - loss: 2.3289 - accuracy: 0.3409 - val_loss: 2.3663 - val_accuracy: 0.3162\n",
            "Epoch 7/200\n",
            "21780/21780 [==============================] - 843s 39ms/step - loss: 2.3230 - accuracy: 0.3421 - val_loss: 2.3672 - val_accuracy: 0.3141\n",
            "Epoch 8/200\n",
            "21780/21780 [==============================] - 842s 39ms/step - loss: 2.3167 - accuracy: 0.3436 - val_loss: 2.3634 - val_accuracy: 0.3172\n",
            "Epoch 9/200\n",
            "21780/21780 [==============================] - 847s 39ms/step - loss: 2.3120 - accuracy: 0.3448 - val_loss: 2.3641 - val_accuracy: 0.3173\n",
            "Epoch 10/200\n",
            "21780/21780 [==============================] - 845s 39ms/step - loss: 2.3113 - accuracy: 0.3452 - val_loss: 2.3661 - val_accuracy: 0.3174\n",
            "Epoch 11/200\n",
            "21780/21780 [==============================] - 853s 39ms/step - loss: 2.3091 - accuracy: 0.3451 - val_loss: 2.3648 - val_accuracy: 0.3195\n",
            "Epoch 12/200\n",
            "21780/21780 [==============================] - 855s 39ms/step - loss: 2.3089 - accuracy: 0.3452 - val_loss: 2.3709 - val_accuracy: 0.3110\n",
            "Epoch 13/200\n",
            " 8478/21780 [==========>...................] - ETA: 7:55 - loss: 2.2948 - accuracy: 0.3474"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXVFpoREhV6Y"
      },
      "source": [
        "### View the Training Graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgkUxCJ52UiZ"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTdXcQCI1-IK"
      },
      "source": [
        "model.save('/content/drive/pass_seq')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeSNfS7uhch0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, 'accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rAgRpxYhjpB"
      },
      "source": [
        "### Non-conditional LSTM Inference\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DC7zfcgviDTp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15c2cd99-1205-4fc8-b1ba-059d23875b2f"
      },
      "source": [
        "seed_texts = [\"\\t\"]\n",
        "\n",
        "num_words=90\n",
        "print(seed_texts)\n",
        "outputs=[]\n",
        "for i,partial in enumerate(seed_texts):\n",
        "  token_list = pass_tokenizer.texts_to_sequences([partial])\n",
        "  email_t= email_tokenizer.texts_to_sequences( [\"chillop.byrne@gmail.com\"] )\n",
        "  email_t=pad_sequences(email_t, max_sequence_len_email)\n",
        "  token_list = pad_sequences(token_list, maxlen=max_sequence_len_pass-1, padding='pre')\n",
        "  \n",
        "  p=model.predict([email_t,token_list])\n",
        "  \n",
        "  ind = np.argpartition(p[0], -num_words)[-num_words:]\n",
        "  # print(p[0][ind])\n",
        "  predicted = np.argmax(p, axis=-1) #\n",
        "  # print(predicted)\n",
        "  # print(ind)\n",
        "  output_word = \"\"\n",
        "  \n",
        "  for w in ind:\n",
        "      if pass_tokenizer.index_word[w] ==  '\\n':\n",
        "        outputs.append(seed_texts[i])\n",
        "        seed_texts.pop(i)\n",
        "      else:\n",
        "        output_word = pass_tokenizer.index_word[w]\n",
        "        seed_texts.append(partial +\"\" + output_word)\n",
        "  #print(seed_texts)\n",
        "  num_words= num_words//2 +1\n",
        "      \n",
        "print(outputs)\n",
        "print(len(outputs))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\tp']\n",
            "['\\tpink', '\\tppink', '\\tppace', '\\tppark', '\\tppoop', '\\tppppp', '\\tpster', '\\tpsppp', '\\tphopo', '\\tpussy', '\\tpurta', '\\tpanda', '\\tpasss', '\\tpoops', '\\tpolly', '\\tpeace', '\\tpenny', '\\tpillo', '\\tplato', '\\tppilit', '\\tppinky', '\\tpprish', '\\tpprint', '\\tpplone', '\\tpploss', '\\tpplast', '\\tpplate', '\\tppacio', '\\tpparky', '\\tppoppy', '\\tppp101', '\\tppp120', '\\tpppp11', '\\tpppppp', '\\tpstris', '\\tpsalia', '\\tpsalla', '\\tpsaris', '\\tpsarda', '\\tpspent', '\\tpspert', '\\tpspper', '\\tpsones', '\\tpsolis', '\\tpsolla', '\\tphanda', '\\tphante', '\\tphaint', '\\tphopon', '\\tphoner', '\\tphista', '\\tphildy', '\\tpusten', '\\tpustic', '\\tpussy1', '\\tpurplo', '\\tpandel', '\\tpantha', '\\tpastri', '\\tpoolie', '\\tpoopin', '\\tpoline', '\\tpolly1', '\\tpearut', '\\tpentor', '\\tpentit', '\\tpilita', '\\tpilina', '\\tpintal', '\\tpintor', '\\tpinked', '\\tplonit', '\\tplondo', '\\tplater', '\\tplayen', '\\tprondo', '\\tpronet', '\\tpritis', '\\tpritty', '\\tprinca', '\\tppilina', '\\tppilito', '\\tppinky1', '\\tpproner', '\\tppronit', '\\tpplonit', '\\tpploss1', '\\tpplates', '\\tpparky1', '\\tppp1010', '\\tpppp110', '\\tpstrist', '\\tpsalina', '\\tpsardan', '\\tpsoness', '\\tphanden', '\\tphandal', '\\tphaints', '\\tphopone', '\\tphonica', '\\tphisser', '\\tphissit', '\\tphildie', '\\tphildy1', '\\tpustens', '\\tpussick', '\\tpussine', '\\tpussy12', '\\tpanda12', '\\tpastang', '\\tpastara', '\\tpoolloo', '\\tpoolies', '\\tpolines', '\\tpearute', '\\tpearlan', '\\tpenny12', '\\tpentita', '\\tpilitis', '\\tpintall', '\\tpinked1', '\\tplonden', '\\tplondon', '\\tplayent', '\\tprocora', '\\tprocher', '\\tpronden', '\\tprondow', '\\tpritis1', '\\tprinten', '\\tprincas', '\\tppilitos', '\\tppilling', '\\tpprochan', '\\tpproner1', '\\tppronica', '\\tpprinten', '\\tpplonico', '\\tpplonity', '\\tpplates1', '\\tppp10100', '\\tpstriste', '\\tpsallang', '\\tpsarista', '\\tpsardana', '\\tpspertin', '\\tpspper12', '\\tpsollang', '\\tphanden1', '\\tphaints1', '\\tphailler', '\\tphonical', '\\tphissine', '\\tphissite', '\\tphildy12', '\\tphilling', '\\tpussick1', '\\tpussy123', '\\tpandelle', '\\tpanda123', '\\tpastanda', '\\tpastara1', '\\tpassswit', '\\tpoollite', '\\tpoollink', '\\tpoollook', '\\tpoopstor', '\\tpoopstan', '\\tpolines1', '\\tpearland', '\\tpeacelov', '\\tpenny123', '\\tpilitisa', '\\tpilina12', '\\tpinked12', '\\tplondon1', '\\tprocolas', '\\tprocorat', '\\tprochang', '\\tprochart', '\\tprondent', '\\tpritis12', '\\tprincass', '\\tppilling1', '\\tpproner12', '\\tpprinten1', '\\tpplonity1', '\\tppp101000', '\\tpsallange', '\\tpsardana1', '\\tpspper123', '\\tphanden12', '\\tphailler1', '\\tphissine1', '\\tphildy123', '\\tpussick12', '\\tpandelle1', '\\tpanda1985', '\\tpastanda1', '\\tpassswitt', '\\tpasssword', '\\tpoopstorn', '\\tpolines12', '\\tpeacelove', '\\tpenny1985', '\\tpilitisa1', '\\tpinked123', '\\tprocolass', '\\tprochange', '\\tprondent1', '\\tprincass1', '\\tpproner123', '\\tpplonity12', '\\tpsallangel', '\\tpspper1234', '\\tphailler12', '\\tphildy1234', '\\tpandelle12', '\\tpastanda12', '\\tpasssword1', '\\tpoollinked', '\\tpoollooker', '\\tpolines123', '\\tpenny19850', '\\tpinked1234', '\\tprochangel', '\\tprincass12', '\\tpplonity123', '\\tpspper12345', '\\tphildy12345', '\\tpastanda123', '\\tpoollookers', '\\tpenny198500', '\\tprochangell', '\\tpplonity1234', '\\tphildy123456', '\\tpoollinkedit', '\\tpoollookers1', '\\tprochangelle', '\\tphildy1234567', '\\tpoollookers12', '\\tphildy12345678', '\\tphildy123456789']\n",
            "243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbJytucIcwHq"
      },
      "source": [
        "from tensorflow import keras\n",
        "model = keras.models.load_model(\"/content/drive/MyDrive/pass_seq\")\n",
        "model.fit(([email_input_sequences,pass_input_sequences]), one_hot_labels, epochs=100, validation_split=0.2, verbose=1, batch_size=32)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwllvEuhjpXn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7e2a55f6-550f-41bb-c21a-6a11232da29f"
      },
      "source": [
        "# from https://keras.io/examples/nlp/lstm_seq2seq/\n",
        "from tensorflow import keras\n",
        "print(model.summary())\n",
        "# encoder_inputs = model.input[0]  # input_1\n",
        "# e0 = model.layers[1](encoder_inputs)  # lstm_1\n",
        "# encoder_outputs, state_h_enc, state_c_enc= model.layers[3].output\n",
        "\n",
        " \n",
        "# encoder_states = [state_h_enc, state_c_enc]\n",
        "# encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "# print(encoder_model.summary())\n",
        "\n",
        "# decoder_inputs = model.input[1]  # input_2\n",
        "# decoder_state_input_h = keras.Input(shape=(latent_dim,), name=\"input_3h\")\n",
        "# decoder_state_input_c = keras.Input(shape=(latent_dim,), name=\"input_4c\")\n",
        "# decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "# decoder_lstm_in = model.layers[4]\n",
        "\n",
        "# d1 = decoder_lstm_in(\n",
        "#     decoder_inputs, initial_state=decoder_states_inputs\n",
        "# )\n",
        "\n",
        "# d1_d2=model.layers[5]\n",
        "# decoder_outputs, state_h_dec, state_c_dec=d1_d2(d1)\n",
        "\n",
        "# decoder_states = [state_h_dec, state_c_dec]\n",
        "# decoder_dense = model.layers[7]\n",
        "# decoder_dropout= model.layers[6]\n",
        "# t=decoder_dropout(decoder_outputs)\n",
        "# decoder_outputs = decoder_dense(t)\n",
        "# decoder_model = keras.Model(\n",
        "#     [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        "# )\n",
        "\n",
        "\n",
        "# print(decoder_model.summary())\n",
        "# # Reverse-lookup token index to decode sequences back to\n",
        "# # something readable.\n",
        "reverse_input_char_index = dict((i, char) for char, i in email_tokenizer.word_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in pass_tokenizer.word_index.items())\n",
        "\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, len(pass_tokenizer.word_index) + 1))\n",
        "    # # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, pass_tokenizer.word_index[\"p\"]] = 1.0\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    seed_texts=[\"\\t\", \" \", \"j\"]\n",
        "    num_words=1\n",
        "    outputs=[]\n",
        "    for i,partial in enumerate(seed_texts):\n",
        "\n",
        "        #target_seq = np.zeros((1, 1, len(pass_tokenizer.word_index) + 1))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "        target_seq[0, 0, pass_tokenizer.word_index[partial[-1]]] = 1.0\n",
        "        states_value = encoder_model.predict(input_seq)\n",
        "        stop_condition=False\n",
        "        j=1\n",
        "        decoded_sentence=\"\"\n",
        "        while not stop_condition:\n",
        "          # token_list = pass_tokenizer.texts_to_sequences([partial])\n",
        "          \n",
        "          # token_list = pad_sequences(token_list, maxlen=max_sequence_len-1, padding='pre')\n",
        "          # p=model.predict(token_list)\n",
        "          \n",
        "          # ind = np.argpartition(p[0], -num_words)[-num_words:]\n",
        "        \n",
        "          # output_word = \"\"\n",
        "      \n",
        "              #num_words= num_words//2 +1\n",
        "\n",
        "              output_tokens = model.predict([input_seq,target_seq])\n",
        "\n",
        "              # Sample a token\n",
        "              #print(output_tokens.shape)\n",
        "              ind = np.argpartition(output_tokens[0,-1,:], -num_words)[-num_words:]\n",
        "\n",
        "              #this is the loop I need to change\n",
        "              sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "              # print(sampled_token_index)\n",
        "              sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "\n",
        "              # for w in ind:\n",
        "              #         if pass_tokenizer.index_word[w] ==  '\\n':\n",
        "              #             outputs.append(seed_texts[i])\n",
        "              #             seed_texts.pop(i)\n",
        "              #             print(\"pop\")\n",
        "              #         else:\n",
        "              #           output_word = pass_tokenizer.index_word[w]\n",
        "              #           seed_texts.append(partial +\"\" + output_word)\n",
        "              decoded_sentence += sampled_char\n",
        "              \n",
        "              #print(decoded_sentence)\n",
        "\n",
        "              # Exit condition: either hit max length\n",
        "              # or find stop character.\n",
        "              if sampled_char=='\\n' or len(decoded_sentence) > max_sequence_len_pass: #not sure if I want this\n",
        "                  stop_condition = True\n",
        "              \n",
        "              # Update the target sequence (of length 1).\n",
        "              # target_seq = np.zeros((1, 1, len(pass_tokenizer.word_index) + 1))\n",
        "              # target_seq[0, 0,sampled_token_index] = 1.0\n",
        "              #multi char seq \n",
        "              target_seq = np.zeros((1, j+1, len(pass_tokenizer.word_index) + 1))\n",
        "              for letter, _ in enumerate(decoded_sentence):\n",
        "                  target_seq[0, letter, pass_tokenizer.word_index[_]] = 1.0\n",
        "\n",
        "              target_seq[0, j, sampled_token_index] = 1.0\n",
        "              j+=1\n",
        "              #states_value = [h, c]\n",
        "        # Update states\n",
        "        #hold state for now\n",
        "        outputs.append(decoded_sentence)\n",
        "              \n",
        "    return outputs\n",
        "\n",
        "for seq_index in range(70000,70003):\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = email_input_sequences[seq_index : seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print(\"-\")\n",
        "    target_seq = np.zeros((1, 3, len(pass_tokenizer.word_index) + 1))\n",
        "    # # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, pass_tokenizer.word_index[\"\\t\"]] = 1.0\n",
        "    target_seq[0, 1, pass_tokenizer.word_index[\"p\"]] = 1.0\n",
        "    target_seq[0, 2, pass_tokenizer.word_index[\"a\"]] = 1.0\n",
        "\n",
        "    test=np.argmax(model.predict([input_seq, target_seq])[0, -1, :] )\n",
        "    #target_seq[0, 1, pass_tokenizer.word_index[\"a\"]] = 1.0\n",
        "    print(pass_tokenizer.index_word[test])\n",
        "    \n",
        "\n",
        "    print(\"Input email:\", email_corpus[seq_index])\n",
        "    print(\"Decoded pass:\", decoded_sentence)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 53)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 53, 256)      21760       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 36)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional (Bidirectional)   [(None, 53, 512), (N 1050624     embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 36, 256)      25600       input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) [(None, 512), (None, 1574912     bidirectional[0][0]              \n",
            "                                                                 bidirectional[0][1]              \n",
            "                                                                 bidirectional[0][2]              \n",
            "                                                                 bidirectional[0][3]              \n",
            "                                                                 bidirectional[0][4]              \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_2 (Bidirectional) (None, 36, 512)      1050624     embedding_1[0][0]                \n",
            "                                                                 bidirectional_1[0][1]            \n",
            "                                                                 bidirectional_1[0][3]            \n",
            "                                                                 bidirectional_1[0][2]            \n",
            "                                                                 bidirectional_1[0][4]            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_3 (Bidirectional) (None, 512)          1574912     bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 512)          0           bidirectional_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 100)          51300       dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 5,349,732\n",
            "Trainable params: 5,349,732\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-72c1f846cf74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;31m# for trying out decoding.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0minput_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memail_input_sequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_index\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mseq_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0mdecoded_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0mtarget_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpass_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-72c1f846cf74>\u001b[0m in \u001b[0;36mdecode_sequence\u001b[0;34m(input_seq)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# Encode the input as state vectors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mstates_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# Generate empty target sequence of length 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'encoder_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hIDo4feClRe"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import Tensor\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "\n",
        "def downsample_residual(x:Tensor, downsample: bool, filters: int, kernel_size: int = 3):\n",
        "    #https://github.com/lidless-vision/keras-residual-vae-tf2.3/blob/43b558f97b74d187ccb8811c3bc7f9e303f5f6c3/vae.py#L94 has batch norm and dropout\n",
        "    y=keras.layers.Conv2D(kernel_size=(kernel_size,kernel_size), strides= (1 if not downsample else 2), filters=filters, padding='same')(x)\n",
        "    print(y.shape)\n",
        "    \n",
        "    y= keras.layers.ReLU()(y)\n",
        "    print(y.shape)\n",
        "    y=keras.layers.Conv2D(kernel_size=(kernel_size,kernel_size), strides= (1,1), filters=filters, padding='same')(y)\n",
        "    print(y.shape)\n",
        "    if downsample:\n",
        "      x=keras.layers.Conv2D(kernel_size=2, strides= 2, filters=filters, )(x)\n",
        "    print(x.shape)\n",
        "    out=keras.layers.Add()([x,y])\n",
        "    out=keras.layers.ReLU()(out)\n",
        "    return out\n",
        "\n",
        "def upsample_residual(x:Tensor, upsample: bool, filters: int, kernel_size: int = 3):\n",
        "    y=keras.layers.Conv2DTranspose(filters=filters, \n",
        "                                   kernel_size=kernel_size,\n",
        "                                   strides= (1 if not upsample else 2),\n",
        "                                   padding='same')(x)\n",
        "    y=keras.layers.ReLU()(y)\n",
        "    y=keras.layers.Conv2DTranspose(filters=filters, \n",
        "                                   kernel_size=kernel_size,\n",
        "                                   strides= 1,\n",
        "                                   padding='same')(y)\n",
        "    if upsample:\n",
        "        x=keras.layers.Conv2DTranspose(filters=filters, \n",
        "                                   kernel_size=2,\n",
        "                                   strides= 2,\n",
        "                                   )(x)\n",
        "    out= keras.layers.Add()([x,y])\n",
        "    out=keras.layers.ReLU()(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "     \n",
        "def define_discriminator(in_shape: [], n_classes: int):\n",
        "\n",
        "    label_in = keras.Input(shape=(1,))\n",
        "\n",
        "    label_embedding = keras.layers.Embedding(n_classes, 50 )(label_in)\n",
        "\n",
        "    n_nodes= tf.reduce_prod(in_shape)\n",
        "    \n",
        "    li= keras.layers.Dense(n_nodes)(label_embedding)\n",
        "\n",
        "    li=keras.layers.Reshape(in_shape)(li)\n",
        "\n",
        "    in_img= keras.Input(shape=in_shape)\n",
        "\n",
        "    merge = keras.layers.Concatenate()([in_img])\n",
        "\n",
        "    ds=downsample_residual(merge, downsample=True, filters=128, kernel_size=3)\n",
        "\n",
        "    flat=keras.layers.Flatten()(ds)\n",
        "\n",
        "    flat=keras.layers.Dropout(0.4)(flat)\n",
        "\n",
        "    belief= keras.layers.Dense(1, activation='sigmoid')(flat)\n",
        "    model= keras.Model([in_img, label_in], belief)\n",
        "    opt= keras.optimizers.Adam(lr=2e-4, beta_1=0.5)\n",
        "    #need to look up gan losses\n",
        "    model.compile(loss= 'binary_crossentropy', optimizer=opt, metrics= ['accuracy'])\n",
        "    return model\n",
        "\n",
        "m=define_discriminator((28,28,1),10)\n",
        "\n",
        "print(m.summary())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}